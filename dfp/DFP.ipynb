{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Goals\n",
    "### An implementation of [Learning to Act by Predicting the Future](https://arxiv.org/pdf/1611.01779.pdf)\n",
    "\n",
    "1. Install requirements with `pip install -r requirements.txt`\n",
    "2. Run this notebook.\n",
    "3. Monitor learning progress with `tensorboard --logdir=worker_0:'./train_0',worker_1:'./train_1',worker_2:'./train_2',worker_3:'./train_3',...worker_n:'./train_n'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import multiprocessing\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "#### Calculating Temporal offsets (f)\n",
    "`get_f` takes a time-series of measurements as well as a set of temporal offsets, and produces the 'f' values for those measurements, which corresponds to how they change in the future at each offset.\n",
    "\n",
    "Given a set of offsets T, and a set of measurements in temporal order m, produces f as follows:\n",
    "\n",
    "`f = <m_T1  – m_0,m_T2  – m_0… m_Tn  – m_0>`\n",
    "\n",
    "\n",
    "#### Experience Buffer\n",
    "`ExperienceBuffer` is used to store a history of experiences that can be randomly drawn from when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f(m,offsets):\n",
    "    f = np.zeros([len(m),m.shape[1],len(offsets)])\n",
    "    for i,offset in enumerate(offsets):\n",
    "        f[:-offset,:,i] = m[offset:,:] - m[:-offset,:]\n",
    "        if i > 0:\n",
    "            f[-offset:,:,i] = f[-offset:,:,i-1]\n",
    "    return f\n",
    "\n",
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(list(self.buffer)) + len(list(experience)) >= self.buffer_size:\n",
    "            self.buffer[0:(len(list(experience))+len(list(self.buffer)))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Direct Future Prediction' (DFP) Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class contain the definition of the neural network in Tensorflow, including the tensorflow ops that will be required for updating the network. In the paper the authors refer to their model as \"Direct Future Prediction (DFP),\" and so I adopt the same notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFP_Network():\n",
    "    def __init__(self,a_size,scope,trainer,num_offsets,num_measurements):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Inputs and visual encoding layers\n",
    "            self.observation = tf.placeholder(shape=[None,5,5,3],dtype=tf.float32)\n",
    "            self.measurements = tf.placeholder(shape=[None,num_measurements],dtype=tf.float32)\n",
    "            self.goals = tf.placeholder(shape=[None,num_measurements],dtype=tf.float32)\n",
    "            self.hidden_o = slim.fully_connected(slim.flatten(self.observation),128,activation_fn=tf.nn.elu)\n",
    "            self.hidden_m = slim.fully_connected(slim.flatten(self.measurements),64,activation_fn=tf.nn.elu)\n",
    "            self.hidden_g = slim.fully_connected(slim.flatten(self.goals),64,activation_fn=tf.nn.elu)\n",
    "            hidden_input = tf.concat([self.hidden_o,self.hidden_m,self.hidden_g],1)\n",
    "            hidden_output = slim.fully_connected(hidden_input,256,activation_fn=tf.nn.elu)\n",
    "\n",
    "            #We calculate separate expectation and advantage streams, then combine then later\n",
    "            #This technique is described in https://arxiv.org/pdf/1511.06581.pdf\n",
    "\n",
    "            self.expectation = slim.fully_connected(hidden_output,a_size * num_offsets * num_measurements,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            self.advantages = slim.fully_connected(hidden_output,a_size * num_offsets * num_measurements,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            \n",
    "            self.advantages = self.advantages - tf.reduce_mean(self.advantages,reduction_indices=1,keep_dims=True)\n",
    "            self.prediction = self.expectation + self.advantages\n",
    "            \n",
    "            #Reshape the predictions to be  [measurements x actions x offsets]\n",
    "            self.prediction = tf.reshape(self.prediction, [-1,num_measurements,a_size,num_offsets])\n",
    "            \n",
    "            # We use a softmax with temperate to pick actions. This is instead of e-greedy.\n",
    "            # For more info on action-selection strategies, see: \n",
    "            # goo.gl/oyL5Vx\n",
    "            self.temperature = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.boltzmann = tf.nn.softmax(tf.reduce_sum(self.prediction,reduction_indices=3)/self.temperature)\n",
    "            \n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "            \n",
    "            # Select the predictions relevant to the chosen action.\n",
    "            self.pred_action = tf.reduce_sum(self.prediction * tf.reshape(self.actions_onehot,[-1,1,a_size,1]), [2])\n",
    "            \n",
    "            #Only the global network need ops for loss functions and gradient updating.\n",
    "            if scope == 'global':\n",
    "                self.target = tf.placeholder(shape=[None,num_measurements,num_offsets],dtype=tf.float32)\n",
    "                \n",
    "                #Loss function\n",
    "                self.loss = tf.reduce_sum(tf.squared_difference(self.pred_action,self.target))\n",
    "                \n",
    "                #Sparsity of the action distribution\n",
    "                self.entropy = -tf.reduce_sum(self.boltzmann * tf.log(self.boltzmann + 1e-7)) \n",
    "\n",
    "                #Get & apply gradients from network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.gradients = tf.gradients(self.loss,global_vars)\n",
    "                self.var_norms = tf.global_norm(global_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,9999.0)\n",
    "                self.apply_grads = trainer.apply_gradients(list(zip(grads,global_vars)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worker Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With asynchronous learning we have multiple 'worker agents,' each of which interacts with their own environment and collects experiences using its own local network. At the end of an episode those experiences are sent to the experience buffer. A random batch of experiences are then drawn from the buffer and the 'global network'  processes them and updates itself with backpropogation. The new global network is then copied over to the worker agent, and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes,offsets,exp_buff,num_measurements,master,gif_path):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.offsets = offsets\n",
    "        self.global_net = master\n",
    "        self.exp_buff = exp_buff\n",
    "        self.model_path = model_path\n",
    "        self.gif_path = gif_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_deliveries = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.num_measurements = num_measurements\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_DFP = DFP_Network(a_size,self.name,trainer,len(offsets),num_measurements)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess):\n",
    "        rollout = np.array(rollout)\n",
    "        measurements = np.vstack(rollout[:,2])\n",
    "        targets = get_f(measurements,self.offsets) #Generate targets using measurements and offsets\n",
    "        rollout[:,4] = list(zip(targets))\n",
    "        self.exp_buff.add(list(zip(rollout)))\n",
    "        \n",
    "        \n",
    "        #Get a batch of experiences from the buffer and use them to update the global network\n",
    "        if len(self.exp_buff.buffer) > 128:\n",
    "            exp_batch = self.exp_buff.sample(128)\n",
    "            feed_dict = {self.global_net.observation:np.stack(exp_batch[:,0],axis=0),\n",
    "                self.global_net.measurements:np.vstack(exp_batch[:,2]),\n",
    "                self.global_net.temperature:[0.1],\n",
    "                self.global_net.actions:exp_batch[:,1],\n",
    "                self.global_net.target:np.vstack(exp_batch[:,4]),\n",
    "                self.global_net.goals:np.vstack(exp_batch[:,3])}\n",
    "            loss,entropy,g_n,v_n,_ = sess.run([self.global_net.loss,\n",
    "                self.global_net.entropy,\n",
    "                self.global_net.grad_norms,\n",
    "                self.global_net.var_norms,\n",
    "                self.global_net.apply_grads],feed_dict=feed_dict)\n",
    "            return loss / len(rollout), entropy / len(rollout), g_n,v_n\n",
    "        else:\n",
    "            return 0,0,0,0\n",
    "        \n",
    "    def work(self,sess,coord,saver,train):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        self.episode_count = episode_count\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops) #Copy parameters from global to local network\n",
    "                episode_buffer = []\n",
    "                episode_frames = []\n",
    "                d = False\n",
    "                t = 0\n",
    "                temp = 0.25 #How spread out we want our action distribution to be\n",
    "                \n",
    "                s,o_big,m,g,h = self.env.reset()\n",
    "                self.the_m = m\n",
    "\n",
    "                while d == False:\n",
    "                    \n",
    "                    #Here is where our goal-switching takes place\n",
    "                    # When the battery charge is below 0.3, we set the goal to optimize battery\n",
    "                    # When the charge is above that value we set the goal to optimize deliveries\n",
    "                    if m[1] <= .3:\n",
    "                        self.g = np.array([0.0,1.0])\n",
    "                    else:\n",
    "                        self.g = np.array([1.0,0.0])\n",
    "                    a_dist = sess.run(self.local_DFP.boltzmann, \n",
    "                        feed_dict={\n",
    "                        self.local_DFP.temperature:[temp],\n",
    "                        self.local_DFP.observation:[s],\n",
    "                        self.local_DFP.measurements:[m],\n",
    "                        self.local_DFP.goals:[self.g]})\n",
    "                    b = self.g*a_dist[0].T\n",
    "                    c = np.sum(b,1)\n",
    "                    c /= c.sum()\n",
    "                    a = np.random.choice(c,p=c)\n",
    "                    a = np.argmax(c == a)\n",
    "                    \n",
    "                    s1,s1_big,m1,g1,h1,d = self.env.step(a)                        \n",
    "                    episode_buffer.append([s,a,np.array(m),self.g,np.zeros(len(self.offsets))])\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        episode_frames.append(set_image_gridworld(s1_big,m1,t+1,g1,h1))\n",
    "                    total_steps += 1\n",
    "                    s = np.copy(s1)\n",
    "                    m = []\n",
    "                    m = m1[:]\n",
    "                    g = g1[:]\n",
    "                    h = h1\n",
    "                    t += 1\n",
    "                    \n",
    "                    # End the episode after 100 steps\n",
    "                    if t > 100:\n",
    "                        d = True\n",
    "                                            \n",
    "                self.episode_deliveries.append(m[0])\n",
    "                self.episode_lengths.append(t)\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if train == True:\n",
    "                    loss,entropy,g_n,v_n = self.train(episode_buffer,sess)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 50 == 0 and episode_count != 0:\n",
    "                    if episode_count % 2000 == 0 and self.name == 'worker_0' and train == True:\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print(\"Saved Model\")\n",
    "\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        time_per_step = 0.25\n",
    "                        self.images = np.array(episode_frames)\n",
    "                        imageio.mimsave(self.gif_path+'/image'+str(episode_count)+'.gif',self.images, duration=time_per_step)\n",
    "                        \n",
    "                    mean_deliveries = np.mean(self.episode_deliveries[-50:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-50:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-50:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Performance/Deliveries', simple_value=float(mean_deliveries))\n",
    "                    summary.value.add(tag='Performance/Length', simple_value=float(mean_length))\n",
    "                    if train == True:\n",
    "                        summary.value.add(tag='Losses/Loss', simple_value=float(loss))\n",
    "                        summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                self.episode_count = episode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_size = 4 # Number of available actions\n",
    "num_measurements = 2 #Number of measurements\n",
    "learning_rate = 1e-3 #Learning ragte\n",
    "offsets = [1,2,4,8,16,32] # Set of temporal offsets\n",
    "load_model = False #Whether to load a saved model\n",
    "train = True #Whether to train the network\n",
    "model_path = './model_goals' #Path to save the model to\n",
    "gif_path = './frames_goals' #Path to save gifs of agent performance to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code establishes the global tensorflow network, as well as creating and starting each of the workers with their own individual networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gameEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d696bce91200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         workers.append(\n\u001b[0;32m---> 20\u001b[0;31m             Worker(gameEnv(partial=False,size=5),i,a_size,\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             exp_buff,num_measurements,master_network,gif_path))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gameEnv' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "exp_buff = ExperienceBuffer()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists(gif_path):\n",
    "    os.makedirs(gif_path)\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "master_network = DFP_Network(a_size,'global',trainer,len(offsets),num_measurements) # Generate global network\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    num_workers = 2 #multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(\n",
    "            Worker(gameEnv(partial=False,size=5),i,a_size,\n",
    "            trainer,model_path,global_episodes,offsets,\n",
    "            exp_buff,num_measurements,master_network,gif_path))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    # Start each of the workers on a separate thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(sess,coord,saver,train)\n",
    "        thread = threading.Thread(target=(worker_work))\n",
    "        thread.start()\n",
    "        time.sleep(0.5)\n",
    "        worker_threads.append(thread)\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
