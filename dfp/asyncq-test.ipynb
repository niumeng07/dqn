{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python2.7/site-packages/matplotlib/__init__.py:1005: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import multiprocessing\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from helper import *\n",
    "from gridworld_rewards import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(list(self.buffer)) + len(list(experience)) >= self.buffer_size:\n",
    "            self.buffer[0:(len(list(experience))+len(list(self.buffer)))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self,a_size,scope,trainer):\n",
    "        with tf.variable_scope(scope):\n",
    "            #Input and visual encoding layers\n",
    "            self.observation = tf.placeholder(shape=[None,5,5,3],dtype=tf.float32)\n",
    "            self.hidden_o = slim.fully_connected(slim.flatten(self.observation),128,activation_fn=tf.nn.elu)\n",
    "            hidden_output = slim.fully_connected(self.hidden_o,256,activation_fn=tf.nn.elu)\n",
    "\n",
    "            #We calculate separate value and advantage streams, then combine then later\n",
    "            #This technique is described in https://arxiv.org/pdf/1511.06581.pdf\n",
    "            self.expectation = slim.fully_connected(hidden_output,a_size,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            self.advantages = slim.fully_connected(hidden_output,a_size,\n",
    "                activation_fn=None,\n",
    "                biases_initializer=None)\n",
    "            self.advantages = self.advantages - tf.reduce_mean(self.advantages,reduction_indices=1,keep_dims=True)\n",
    "            self.prediction = self.expectation + self.advantages\n",
    "            \n",
    "            # We use a softmax with temperate to pick actions. This is instead of e-greedy.\n",
    "            # For more info on action-selection strategies, see: \n",
    "            # goo.gl/oyL5Vx\n",
    "            \n",
    "            self.temperature = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "            self.boltzmann = tf.nn.softmax(self.prediction/self.temperature)\n",
    "            \n",
    "            self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "            self.actions_onehot = tf.one_hot(self.actions,a_size,dtype=tf.float32)\n",
    "            \n",
    "            self.pred_action = tf.reduce_sum(self.prediction * self.actions_onehot, [1])\n",
    "            \n",
    "            #Only the global network need ops for loss functions and gradient updating.\n",
    "            if scope == 'global':\n",
    "                self.target = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "                \n",
    "                #Loss function\n",
    "                self.loss = tf.reduce_sum(tf.squared_difference(self.pred_action,self.target))\n",
    "                \n",
    "                #Entropy tells us how diverse our action probabilities are\n",
    "                self.entropy = -tf.reduce_sum(self.boltzmann * tf.log(self.boltzmann + 1e-7))\n",
    "\n",
    "                #Get gradients from network\n",
    "                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.gradients = tf.gradients(self.loss,global_vars)\n",
    "                self.var_norms = tf.global_norm(global_vars)\n",
    "                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,9999.0)\n",
    "                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self,game,name,a_size,trainer,model_path,global_episodes,exp_buff,master,gif_path):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name        \n",
    "        self.global_net = master\n",
    "        self.exp_buff = exp_buff\n",
    "        self.model_path = model_path\n",
    "        self.gif_path = gif_path\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_deliveries = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\"train_\"+str(self.number))\n",
    "\n",
    "        #Create the local copy of the network and the tensorflow op to copy global paramters to local network\n",
    "        self.local_Q = QNetwork(a_size,self.name,trainer)\n",
    "        self.update_local_ops = update_target_graph('global',self.name)        \n",
    "        self.env = game\n",
    "        \n",
    "    def train(self,rollout,sess):\n",
    "        rollout = np.array(rollout)\n",
    "        self.exp_buff.add(zip(rollout))\n",
    "        \n",
    "        if len(self.exp_buff.buffer) > 128:\n",
    "            exp_batch = self.exp_buff.sample(128)\n",
    "            feed_dict = {self.global_net.observation:np.stack(exp_batch[:,0],axis=0),\n",
    "                self.global_net.actions:exp_batch[:,1],\n",
    "                self.global_net.target:exp_batch[:,2]}\n",
    "            loss,g_n,v_n,_ = sess.run([self.global_net.loss,\n",
    "                self.global_net.grad_norms,\n",
    "                self.global_net.var_norms,\n",
    "                self.global_net.apply_grads],feed_dict=feed_dict)\n",
    "            return loss / len(rollout), g_n,v_n\n",
    "        else:\n",
    "            return 0,0,0\n",
    "        \n",
    "    def work(self,sess,coord,saver,train):\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        self.episode_count = episode_count\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():                 \n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_frames = []\n",
    "                episode_rewards = 0\n",
    "                d = False\n",
    "                t = 0\n",
    "                temp = 0.25\n",
    "                \n",
    "                s,o_big,m,g,h = self.env.reset()\n",
    "\n",
    "                while d == False:\n",
    "                    a_dist = sess.run(self.local_Q.boltzmann, \n",
    "                        feed_dict={self.local_Q.observation:[s],self.local_Q.temperature:[temp]})\n",
    "                    a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                    a = np.argmax(a_dist[0] == a)\n",
    "                    \n",
    "                    s1,s1_big,r,g1,h1,d = self.env.step(a)      \n",
    "                    \n",
    "                    #The Q-learning update rule\n",
    "                    # We use this to generate target values to update our Q-network toward\n",
    "                    if d == True:\n",
    "                        y = r\n",
    "                    else:\n",
    "                        self.qnext = sess.run(self.local_Q.prediction,feed_dict={self.local_Q.observation:[s1]})\n",
    "                        y = r + 0.95*np.max(self.qnext)\n",
    "                    \n",
    "                    episode_rewards += r\n",
    "                    episode_buffer.append([s,a,y])\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        episode_frames.append(set_image_gridworld_reward(s1_big,episode_rewards,t+1,g1,h1))\n",
    "                    total_steps += 1\n",
    "                    s = s1\n",
    "                    g = g1\n",
    "                    h = h1\n",
    "                    t += 1\n",
    "                    \n",
    "                    if t > 100:\n",
    "                        d = True\n",
    "                                            \n",
    "                self.episode_deliveries.append(episode_rewards)\n",
    "                self.episode_lengths.append(t)\n",
    "                \n",
    "                # Update the network using the experience buffer at the end of the episode.\n",
    "                if train == True:\n",
    "                    loss,g_n,v_n = self.train(episode_buffer,sess)\n",
    "            \n",
    "                    \n",
    "                # Periodically save gifs of episodes, model parameters, and summary statistics.\n",
    "                if episode_count % 50 == 0 and episode_count != 0:\n",
    "                    if episode_count % 2000 == 0 and self.name == 'worker_0' and train == True:\n",
    "                        saver.save(sess,self.model_path+'/model-'+str(episode_count)+'.cptk')\n",
    "                        print(\"Saved Model\")\n",
    "\n",
    "                    if self.name == 'worker_0' and episode_count % 150 == 0:\n",
    "                        time_per_step = 0.25\n",
    "                        self.images = np.array(episode_frames)\n",
    "                        imageio.mimsave(self.gif_path+'/image'+str(episode_count)+'.gif',self.images, duration=time_per_step)\n",
    "                        \n",
    "                    mean_deliveries = np.mean(self.episode_deliveries[-50:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-50:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-50:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Performance/Deliveries', simple_value=float(mean_deliveries))\n",
    "                    if train == True:\n",
    "                        summary.value.add(tag='Losses/Loss', simple_value=float(loss))\n",
    "                        summary.value.add(tag='Losses/Grad Norm', simple_value=float(g_n))\n",
    "                    self.summary_writer.add_summary(summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "                self.episode_count = episode_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_size = 4 # Number of available actions\n",
    "load_model = False #Whether to load the model or start training from scratch\n",
    "train = True #Whether to train the model or simply use it for solving the task\n",
    "model_path = './model_Q' #The location to save the model to\n",
    "gif_path = './frames_Q' #The location to save gifs of the agent-environemnt interaction to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config ready.\n",
      "device ready.\n",
      "initialize ready.\n",
      "Starting worker 0\n",
      "Starting worker 1\n",
      "workers ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python2.7/threading.py\", line 812, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python2.7/threading.py\", line 765, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"<ipython-input-10-2d9a8f3b946c>\", line 42, in <lambda>\n",
      "    worker_work = lambda: worker.work(sess,coord,saver,train)\n",
      "  File \"<ipython-input-5-70aa44689c72>\", line 102, in work\n",
      "    imageio.mimsave(self.gif_path+'/image'+str(episode_count)+'.gif',self.images, duration=time_per_step)\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/core/functions.py\", line 316, in mimwrite\n",
      "    writer = get_writer(uri, format, 'I', **kwargs)\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/core/functions.py\", line 166, in get_writer\n",
      "    format = formats.search_write_format(request)\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/core/format.py\", line 688, in search_write_format\n",
      "    if format.can_write(request):\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/core/format.py\", line 196, in can_write\n",
      "    return self._can_write(request)\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/plugins/pillow.py\", line 75, in _can_write\n",
      "    Image = self._init_pillow()\n",
      "  File \"/usr/lib/python2.7/site-packages/imageio/plugins/pillow.py\", line 55, in _init_pillow\n",
      "    raise RuntimeError('Imageio Pillow plugin requires '\n",
      "RuntimeError: Imageio Pillow plugin requires Pillow lib.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "exp_buff = ExperienceBuffer()\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    \n",
    "if not os.path.exists(gif_path):\n",
    "    os.makedirs(gif_path)\n",
    "\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "master_network = QNetwork(a_size,'global',trainer) # Generate global network\n",
    "print \"config ready.\"\n",
    "with tf.device(\"/cpu:0\"): \n",
    "    global_episodes = tf.Variable(0,dtype=tf.int32,name='global_episodes',trainable=False)\n",
    "    num_workers = 2 # Set workers ot number of available CPU threads\n",
    "    workers = []\n",
    "    # Create worker classes\n",
    "    for i in range(num_workers):\n",
    "        workers.append(\n",
    "        Worker(gameEnv(partial=False,size=5),\n",
    "        i,a_size,trainer,model_path,global_episodes,\n",
    "        exp_buff,master_network,gif_path))\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "print\"device ready.\"\n",
    "config = tf.ConfigProto()             \n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"initialize ready.\")\n",
    "    # Start each of the workers on a separate thread\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        worker_work = lambda: worker.work(sess,coord,saver,train)\n",
    "        thread = threading.Thread(target=(worker_work))\n",
    "        thread.start()\n",
    "        time.sleep(0.5)\n",
    "        worker_threads.append(thread)\n",
    "    print(\"workers ready.\")\n",
    "    coord.join(worker_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
